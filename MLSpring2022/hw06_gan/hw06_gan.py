# -*- coding: utf-8 -*-
"""ML_HW6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10lHBPFoNhTiiPe-yZ7SwAV1wwrkGc4En

# Homework 6 - Generative Adversarial Network
This is the sample code for hw6 of 2022 Machine Learning course in National Taiwan University.

In this sample code, there are 5 sections:
1. Environment setting
2. Dataset preparation
3. Model setting
4. Train
5. Inference

Your goal is to do anime face generation, if you have any question, please discuss at NTU COOL

# Environment setting
In this section, we will prepare for the dataset and set some environment variable

## Download Dataset
"""

import glob
import json
import logging
import os
import random
from datetime import datetime

import matplotlib.pyplot as plt
import numpy as np
import torch
import torchvision
from torch import nn, optim
from torch.autograd import Variable
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
from tqdm import tqdm

# seed setting
def same_seeds(seed):
    # Python built-in random module
    random.seed(seed)
    # Numpy
    np.random.seed(seed)
    # Torch
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

"""# Dataset preparation
In this section, we prepare for the dataset for Pytorch

## Create dataset for Pytorch

In order to unified image information, we use the transform function to:
1. Resize image to 64x64
2. Normalize the image

This CrypkoDataset class will be use in Section 4
"""

# prepare for CrypkoDataset

class CrypkoDataset(Dataset):
    def __init__(self, fnames, transform):
        self.transform = transform
        self.fnames = fnames
        self.num_samples = len(self.fnames)

    def __getitem__(self,idx):
        fname = self.fnames[idx]
        img = torchvision.io.read_image(fname)
        img = self.transform(img)
        return img

    def __len__(self):
        return self.num_samples

def get_dataset(root: str) -> CrypkoDataset:
    fnames = glob.glob(os.path.join(root, '*'))

    compose = [
        transforms.ToPILImage(),
        transforms.Resize((64, 64)),
        transforms.ToTensor(),
        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
    ]

    transform = transforms.Compose(compose)

    return CrypkoDataset(fnames, transform)

class Generator(nn.Module):
    """
    Input shape: (batch, in_dim)
    Output shape: (batch, 3, 64, 64)
    """
    def __init__(self, in_dim, feature_dim=64):
        super().__init__()

        #input: (batch, 100)
        self.l1 = nn.Sequential(
            nn.Linear(in_dim, feature_dim * 8 * 4 * 4, bias=False),
            nn.BatchNorm1d(feature_dim * 8 * 4 * 4),
            nn.ReLU()
        )
        self.l2 = nn.Sequential(
            self.dconv_bn_relu(feature_dim * 8, feature_dim * 4),               #(batch, feature_dim * 16, 8, 8)
            self.dconv_bn_relu(feature_dim * 4, feature_dim * 2),               #(batch, feature_dim * 16, 16, 16)
            self.dconv_bn_relu(feature_dim * 2, feature_dim),                   #(batch, feature_dim * 16, 32, 32)
        )
        self.l3 = nn.Sequential(
            nn.ConvTranspose2d(feature_dim, 3, kernel_size=5, stride=2,
                               padding=2, output_padding=1, bias=False),
            nn.Tanh()
        )
        self.apply(weights_init)

    def dconv_bn_relu(self, in_dim, out_dim):
        return nn.Sequential(
            nn.ConvTranspose2d(in_dim, out_dim, kernel_size=5, stride=2,
                               padding=2, output_padding=1, bias=False),        #double height and width
            nn.BatchNorm2d(out_dim),
            nn.ReLU(True)
        )

    def forward(self, x):
        y = self.l1(x)
        y = y.view(y.size(0), -1, 4, 4)
        y = self.l2(y)
        y = self.l3(y)

        return y

# Discriminator
class Discriminator(nn.Module):
    """
    Input shape: (batch, 3, 64, 64)
    Output shape: (batch)
    """
    def __init__(self, in_dim, feature_dim=64):
        super(Discriminator, self).__init__()

        #input: (batch, 3, 64, 64)
        """
        NOTE FOR SETTING DISCRIMINATOR:

        Remove last sigmoid layer for WGAN
        """
        self.l1 = nn.Sequential(
            nn.Conv2d(in_dim, feature_dim, kernel_size=4, stride=2, padding=1), #(batch, 3, 32, 32)
            nn.LeakyReLU(0.2),
            self.conv_bn_lrelu(feature_dim, feature_dim * 2),                   #(batch, 3, 16, 16)
            self.conv_bn_lrelu(feature_dim * 2, feature_dim * 4),               #(batch, 3, 8, 8)
            self.conv_bn_lrelu(feature_dim * 4, feature_dim * 8),               #(batch, 3, 4, 4)
            nn.Conv2d(feature_dim * 8, 1, kernel_size=4, stride=1, padding=0),
            nn.Sigmoid()
        )
        self.apply(weights_init)

    def conv_bn_lrelu(self, in_dim, out_dim):
        """
        NOTE FOR SETTING DISCRIMINATOR:

        You can't use nn.Batchnorm for WGAN-GP
        Use nn.InstanceNorm2d instead
        """

        return nn.Sequential(
            nn.Conv2d(in_dim, out_dim, 4, 2, 1),
            nn.BatchNorm2d(out_dim),
            nn.LeakyReLU(0.2),
        )

    def forward(self, x):
        y = self.l1(x)
        y = y.view(-1)
        return y

# setting for weight init function
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        m.weight.data.normal_(0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)

class Trainer():
    def __init__(self, config: dict):
        self.config = config

        self.dataloader = None
        self.log_dir = os.path.join(self.config["workspace_dir"], 'logs')
        self.ckpt_dir = os.path.join(self.config["workspace_dir"], 'checkpoints')

        log_format = '%(asctime)s - %(levelname)s: %(message)s'
        logging.basicConfig(level=logging.INFO, format=log_format, datefmt='%Y-%m-%d %H:%M')

        self.steps = 0

        self.generator = Generator(config['z_dim'])
        self.discriminator = Discriminator(3)

        self.loss = nn.BCELoss()

        """
        NOTE FOR SETTING OPTIMIZER:

        GAN: use Adam optimizer
        WGAN: use RMSprop optimizer
        WGAN-GP: use Adam optimizer
        """
        self.opt_D = optim.Adam(
            self.discriminator.parameters(), lr=self.config["lr"], betas=(0.5, 0.999)
        )
        self.opt_G = optim.Adam(
            self.generator.parameters(), lr=self.config["lr"], betas=(0.5, 0.999)
        )
        self.z_samples = Variable(torch.randn(100, self.config["z_dim"])).cuda()

    def prepare_environment(self):
        """ Use this function to prepare environment """
        os.makedirs(self.log_dir, exist_ok=True)
        os.makedirs(self.ckpt_dir, exist_ok=True)

        # update dir by time
        time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        self.log_dir = os.path.join(self.log_dir, f'{time}_{self.config["model_type"]}')
        self.ckpt_dir = os.path.join(self.ckpt_dir, f'{time}_{self.config["model_type"]}')

        os.makedirs(self.log_dir)
        os.makedirs(self.ckpt_dir)

        # Store hyperparameters
        with open(os.path.join(self.log_dir, 'config.json'), 'w') as f:
            json.dump(self.config, f, ensure_ascii=True, indent=4)

        # create dataset by the above function
        dataset = get_dataset(os.path.join(self.config["workspace_dir"], 'hw06_data'))
        self.dataloader = DataLoader(dataset, batch_size=self.config["batch_size"], shuffle=True, num_workers=2)

        # model preparation
        self.generator = self.generator.cuda()
        self.discriminator = self.discriminator.cuda()
        self.generator.train()
        self.discriminator.train()

    def gradient_peanlty(self):
        """ Implement gradient penalty function """
        pass

    def train(self):
        """ Use this function to train generator and discriminator """
        self.prepare_environment()

        loss_discriminator = []
        loss_generator = []
        traces = []

        for epoch in range(self.config["n_epoch"]):
            progress_bar = tqdm(self.dataloader)
            progress_bar.set_description(f"Epoch {epoch+1}")

            for i, data in enumerate(progress_bar):
                imgs = data.cuda()
                batch_size = imgs.size(0)

                # *********************
                # *    Train D        *
                # *********************
                z = Variable(torch.randn(batch_size, self.config["z_dim"])).cuda()
                r_imgs = Variable(imgs).cuda()
                f_imgs = self.generator(z)
                r_label = torch.ones((batch_size)).cuda()
                f_label = torch.zeros((batch_size)).cuda()

                # Discriminator forwarding
                r_logit = self.discriminator(r_imgs)
                f_logit = self.discriminator(f_imgs)

                """
                NOTE FOR SETTING DISCRIMINATOR LOSS:

                GAN:
                    loss_D = (r_loss + f_loss)/2
                WGAN:
                    loss_D = -torch.mean(r_logit) + torch.mean(f_logit)
                WGAN-GP:
                    gradient_penalty = self.gp(r_imgs, f_imgs)
                    loss_D = -torch.mean(r_logit) + torch.mean(f_logit) + gradient_penalty
                """
                # Loss for discriminator
                r_loss = self.loss(r_logit, r_label)
                f_loss = self.loss(f_logit, f_label)
                loss_D = (r_loss + f_loss) / 2

                # Discriminator backwarding
                self.discriminator.zero_grad()
                loss_D.backward()
                self.opt_D.step()

                """
                NOTE FOR SETTING WEIGHT CLIP:

                WGAN: below code
                """
                # for p in self.D.parameters():
                #     p.data.clamp_(-self.config["clip_value"], self.config["clip_value"])

                # *********************
                # *    Train G        *
                # *********************
                if self.steps % self.config["n_critic"] == 0:
                    # Generate some fake images.
                    z = Variable(torch.randn(batch_size, self.config["z_dim"])).cuda()
                    f_imgs = self.generator(z)

                    # Generator forwarding
                    f_logit = self.discriminator(f_imgs)

                    """
                    NOTE FOR SETTING LOSS FOR GENERATOR:

                    GAN: loss_G = self.loss(f_logit, r_label)
                    WGAN: loss_G = -torch.mean(self.D(f_imgs))
                    WGAN-GP: loss_G = -torch.mean(self.D(f_imgs))
                    """
                    # Loss for the generator.
                    loss_G = self.loss(f_logit, r_label)

                    # Generator backwarding
                    self.generator.zero_grad()
                    loss_G.backward()
                    self.opt_G.step()

                if self.steps % 10 == 0:
                    progress_bar.set_postfix(loss_G=loss_G.item(), loss_D=loss_D.item())
                    loss_discriminator.append(loss_D.item())
                    loss_generator.append(loss_G.item())
                    traces.append(self.steps)

                self.steps += 1

            fig, ax1 = plt.subplots(figsize=(12.8, 7.2))
            ax2 = ax1.twinx()

            ax1.plot(traces, loss_discriminator, 'r', label='discriminator')
            ax2.plot(traces, loss_generator, 'b', label='generator')
            ax1.set_xlabel('iterations')
            ax1.set_ylabel('discriminator')
            ax2.set_ylabel('generator')
            plt.savefig(os.path.join(self.log_dir, 'Loss.jpg'))
            plt.clf()

            self.generator.eval()
            f_imgs_sample = (self.generator(self.z_samples).data + 1) / 2.0
            filename = os.path.join(self.log_dir, f'Epoch_{epoch+1:03d}.jpg')
            torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)
            logging.info(f'Save some samples to {filename}.')

            # Show some images during training.
            grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)
            # plt.figure(figsize=(10,10))
            # plt.imshow(grid_img.permute(1, 2, 0))
            # plt.show()

            self.generator.train()

            # Save the checkpoints.
            if (epoch + 1) % 5 == 0:
                torch.save(self.generator.state_dict(), os.path.join(self.ckpt_dir, f'G_{epoch}.pth'))
                torch.save(self.discriminator.state_dict(), os.path.join(self.ckpt_dir, f'D_{epoch}.pth'))

        logging.info('Finish training')

    def inference(self, G_path, n_generate=1000, n_output=30, show=False):
        """
        1. G_path is the path for Generator ckpt
        2. You can use this function to generate final answer
        """

        self.generator.load_state_dict(torch.load(G_path))
        self.generator.cuda()
        self.generator.eval()
        z = Variable(torch.randn(n_generate, self.config["z_dim"])).cuda()
        imgs = (self.generator(z).data + 1) / 2.0

        os.makedirs('output', exist_ok=True)
        for i in range(n_generate):
            torchvision.utils.save_image(imgs[i], f'output/{i+1}.jpg')

        if show:
            row, col = n_output//10 + 1, 10
            grid_img = torchvision.utils.make_grid(imgs[:n_output].cpu(), nrow=row)
            plt.figure(figsize=(row, col))
            plt.imshow(grid_img.permute(1, 2, 0))
            plt.show()

def main():
    same_seeds(2022)

    # temp_dataset = get_dataset(os.path.join(workspace_dir, 'hw06_data'))

    # images = [temp_dataset[i] for i in range(4)]
    # grid_img = torchvision.utils.make_grid(images, nrow=4)
    # plt.figure(figsize=(10,10))
    # plt.imshow(grid_img.permute(1, 2, 0))
    # plt.show()

    config = {
        "model_type": "GAN",
        "batch_size": 1024,
        "lr": 1e-4,
        "n_epoch": 30,
        "n_critic": 1,
        "z_dim": 100,
        "workspace_dir": '.',
    }

    trainer = Trainer(config)
    trainer.train()

    # save the 1000 images into ./output folder
    # workspace_dir = config['workspace_dir']
    # trainer.inference(f'{workspace_dir}/checkpoints/2022-04-11_15-38-56_GAN/G_9.pth')

    return

if __name__ == "__main__":
    main()
