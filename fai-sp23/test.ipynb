{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(313)\n",
    "\n",
    "\"\"\"\n",
    "Tips for debugging:\n",
    "- Use `print` to check the shape of your data. Shape mismatch is a common error.\n",
    "- Use `ipdb` to debug your code\n",
    "    - `ipdb.set_trace()` to set breakpoints and check the values of your variables in interactive mode\n",
    "    - `python -m ipdb -c continue hw3.py` to run the entire script in debug mode. Once the script is paused, you can use `n` to step through the code line by line.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 1. Load datasets\n",
    "def load_data() -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    DO NOT MODIFY THIS FUNCTION.\n",
    "    \"\"\"\n",
    "    # Load iris dataset\n",
    "    iris = pd.read_csv(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\n",
    "        header=None,\n",
    "    )\n",
    "    iris.columns = [\n",
    "        \"sepal_length\",\n",
    "        \"sepal_width\",\n",
    "        \"petal_length\",\n",
    "        \"petal_width\",\n",
    "        \"class\",\n",
    "    ]\n",
    "\n",
    "    # Load Boston housing dataset\n",
    "    boston = pd.read_csv(\n",
    "        \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\"\n",
    "    )\n",
    "\n",
    "    return iris, boston\n",
    "\n",
    "\n",
    "# 2. Preprocessing functions\n",
    "def train_test_split(\n",
    "    df: pd.DataFrame, target: str, test_size: float = 0.3\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # Shuffle and split dataset into train and test sets\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    train = df.iloc[:split_idx]\n",
    "    test = df.iloc[split_idx:]\n",
    "\n",
    "    # Split target and features\n",
    "    X_train = train.drop(target, axis=1).values\n",
    "    y_train = train[target].values\n",
    "    X_test = test.drop(target, axis=1).values\n",
    "    y_test = test[target].values\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def normalize(X: np.ndarray) -> np.ndarray:\n",
    "    # Normalize features to [0, 1]\n",
    "    # You can try other normalization methods, e.g., z-score, etc.\n",
    "    # TODO: 1%\n",
    "    X_min = np.min(X, axis=0)\n",
    "    X_max = np.max(X, axis=0)\n",
    "    X_norm = (X - X_min) / (X_max - X_min)\n",
    "    return X_norm\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def one_hot_encode(y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    One-hot encode labels.\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(y)\n",
    "    num_classes = len(unique_labels)\n",
    "    encoded_labels = np.zeros((len(y), num_classes))\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        encoded_labels[y == label, i] = 1\n",
    "    return encoded_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # y_true = np.argmax(y_true, axis=1 )\n",
    "    print(f'y_true : {y_true}\\ny_pred : {y_pred}' )\n",
    "    correct = np.sum(y_true == y_pred)    \n",
    "    total = len(y_true)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def encode_labels(y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Encode labels to integers.\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(y)\n",
    "    label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "    encoded_labels = np.array([label_map[label] for label in y])\n",
    "    return encoded_labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0 1 2 2 1 2 1 2 1 0 2 1 0 0 0 1 2 0 0 0 1 0 1 2 0 1 2 0 2 2\n",
      " 1 1 2 1 0 1 2 0 0 1 1 0 2 0 0 1 1 2 1 2 2 1 0 0 2 2 0 0 0 1 2] (105,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iris, boston = load_data()\n",
    "\n",
    "# Iris dataset - Classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris, \"class\")\n",
    "X_train, X_test = normalize(X_train), normalize(X_test)\n",
    "# print(y_train)\n",
    "y_train, y_test = encode_labels(y_train), encode_labels(y_test)\n",
    "print(y_train, y_train.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    def __init__(self, learning_rate=0.01, iterations=15, model_type=\"linear\") -> None:\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.model_type = model_type\n",
    "\n",
    "        assert model_type in [\"linear\", \"logistic\"], \"model_type must be either 'linear' or 'logistic'\"\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        n_classes = len(np.unique(y))\n",
    "        n_features = X.shape[1]\n",
    "        self.weights = np.zeros((n_features, n_classes))\n",
    "\n",
    "        if self.model_type == \"logistic\":\n",
    "            y = y.reshape(-1, 1)\n",
    "            for iter in range(self.iterations):\n",
    "                gradients = self._compute_gradients(X, y, iter )\n",
    "                print(f'{gradients=}')\n",
    "                self.weights -= self.learning_rate * gradients\n",
    "        else:\n",
    "            for iter in range(self.iterations):\n",
    "                gradients = self._compute_gradients(X, y, iter)\n",
    "                self.weights += self.learning_rate * gradients\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        if self.model_type == \"linear\":\n",
    "            return np.dot(X, self.weights)\n",
    "        elif self.model_type == \"logistic\":\n",
    "            prob = self._softmax(np.dot(X, self.weights))\n",
    "            print(\"prob : \" , prob)\n",
    "            return np.argmax(prob, axis= 1)\n",
    "\n",
    "    def _compute_gradients(self, X: np.ndarray, y: np.ndarray, iter ) -> np.ndarray:\n",
    "        if self.model_type == \"linear\":\n",
    "            predictions = np.dot(X, self.weights)\n",
    "            error = predictions - y\n",
    "            gradients = np.dot(X.T, error) / len(y)\n",
    "        elif self.model_type == \"logistic\":\n",
    "            Y = np.zeros((len(y), 3))\n",
    "            for i in range(3):\n",
    "                Y[y.flatten() == i, i] = 1\n",
    "            \n",
    "            predictions = self._softmax(np.dot(X, self.weights))\n",
    "            error = predictions - Y\n",
    "            gradients = np.dot(X.T, error) / len(y)\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def _softmax(self, z: np.ndarray) -> np.ndarray:\n",
    "        exp = np.exp(z)\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients=array([[-0.04761905,  0.01904762,  0.02857143],\n",
      "       [ 0.06261023, -0.00246914, -0.06014109],\n",
      "       [-0.07804233,  0.0505291 ,  0.02751323],\n",
      "       [ 0.11665321, -0.02684961, -0.08980361],\n",
      "       [ 0.11997354, -0.01812169, -0.10185185]])\n",
      "gradients=array([[-0.04777279,  0.01897739,  0.02879541],\n",
      "       [ 0.06248486, -0.00248711, -0.05999775],\n",
      "       [-0.078081  ,  0.05048771,  0.02759329],\n",
      "       [ 0.11649106, -0.02686159, -0.08962947],\n",
      "       [ 0.11980938, -0.01813262, -0.10167676]])\n",
      "gradients=array([[-0.04792521,  0.0189073 ,  0.02901791],\n",
      "       [ 0.06236015, -0.00250507, -0.05985508],\n",
      "       [-0.0781191 ,  0.0504464 ,  0.0276727 ],\n",
      "       [ 0.11632964, -0.02687356, -0.08945608],\n",
      "       [ 0.11964594, -0.01814355, -0.10150239]])\n",
      "gradients=array([[-0.04807629,  0.01883736,  0.02923893],\n",
      "       [ 0.06223609, -0.00252301, -0.05971308],\n",
      "       [-0.07815664,  0.05040517,  0.02775147],\n",
      "       [ 0.11616896, -0.02688552, -0.08928343],\n",
      "       [ 0.11948322, -0.01815447, -0.10132875]])\n",
      "gradients=array([[-0.04822606,  0.01876756,  0.0294585 ],\n",
      "       [ 0.06211268, -0.00254093, -0.05957175],\n",
      "       [-0.07819361,  0.05036401,  0.02782959],\n",
      "       [ 0.116009  , -0.02689747, -0.08911152],\n",
      "       [ 0.11932123, -0.01816539, -0.10115584]])\n",
      "gradients=array([[-0.04837451,  0.01869791,  0.02967661],\n",
      "       [ 0.06198991, -0.00255882, -0.05943109],\n",
      "       [-0.07823002,  0.05032294,  0.02790708],\n",
      "       [ 0.11584976, -0.02690941, -0.08894034],\n",
      "       [ 0.11915995, -0.0181763 , -0.10098365]])\n",
      "gradients=array([[-0.04852167,  0.0186284 ,  0.02989326],\n",
      "       [ 0.06186778, -0.0025767 , -0.05929108],\n",
      "       [-0.07826587,  0.05028195,  0.02798392],\n",
      "       [ 0.11569124, -0.02692134, -0.08876989],\n",
      "       [ 0.11899938, -0.0181872 , -0.10081218]])\n",
      "gradients=array([[-0.04866753,  0.01855905,  0.03010848],\n",
      "       [ 0.06174629, -0.00259455, -0.05915174],\n",
      "       [-0.07830117,  0.05024103,  0.02806014],\n",
      "       [ 0.11553343, -0.02693326, -0.08860017],\n",
      "       [ 0.11883952, -0.01819809, -0.10064143]])\n",
      "gradients=array([[-0.0488121 ,  0.01848985,  0.03032226],\n",
      "       [ 0.06162542, -0.00261237, -0.05901305],\n",
      "       [-0.07833592,  0.0502002 ,  0.02813572],\n",
      "       [ 0.11537633, -0.02694515, -0.08843117],\n",
      "       [ 0.11868036, -0.01820897, -0.10047139]])\n",
      "gradients=array([[-0.0489554 ,  0.0184208 ,  0.0305346 ],\n",
      "       [ 0.06150519, -0.00263017, -0.05887502],\n",
      "       [-0.07837012,  0.05015944,  0.02821068],\n",
      "       [ 0.11521993, -0.02695703, -0.0882629 ],\n",
      "       [ 0.1185219 , -0.01821984, -0.10030206]])\n",
      "gradients=array([[-0.04909743,  0.0183519 ,  0.03074553],\n",
      "       [ 0.06138557, -0.00264794, -0.05873763],\n",
      "       [-0.07840378,  0.05011877,  0.02828501],\n",
      "       [ 0.11506424, -0.0269689 , -0.08809534],\n",
      "       [ 0.11836413, -0.01823069, -0.10013344]])\n",
      "gradients=array([[-0.04923819,  0.01828316,  0.03095503],\n",
      "       [ 0.06126658, -0.00266568, -0.0586009 ],\n",
      "       [-0.0784369 ,  0.05007818,  0.02835872],\n",
      "       [ 0.11490924, -0.02698074, -0.0879285 ],\n",
      "       [ 0.11820705, -0.01824152, -0.09996553]])\n",
      "gradients=array([[-0.0493777 ,  0.01821457,  0.03116313],\n",
      "       [ 0.06114821, -0.00268339, -0.05846481],\n",
      "       [-0.07846948,  0.05003767,  0.02843181],\n",
      "       [ 0.11475493, -0.02699256, -0.08776238],\n",
      "       [ 0.11805066, -0.01825234, -0.09979832]])\n",
      "gradients=array([[-0.04951597,  0.01814615,  0.03136982],\n",
      "       [ 0.06103044, -0.00270108, -0.05832937],\n",
      "       [-0.07850153,  0.04999725,  0.02850428],\n",
      "       [ 0.11460132, -0.02700436, -0.08759696],\n",
      "       [ 0.11789495, -0.01826314, -0.09963181]])\n",
      "gradients=array([[-0.049653  ,  0.01807788,  0.03157512],\n",
      "       [ 0.06091329, -0.00271873, -0.05819456],\n",
      "       [-0.07853305,  0.0499569 ,  0.02857614],\n",
      "       [ 0.11444838, -0.02701613, -0.08743225],\n",
      "       [ 0.11773991, -0.01827392, -0.09946599]])\n",
      "prob :  [[0.33748364 0.33090882 0.33160754]\n",
      " [0.32514151 0.33330007 0.34155841]\n",
      " [0.32480931 0.33311096 0.34207973]\n",
      " [0.33727632 0.3311442  0.33157948]\n",
      " [0.32827225 0.33297998 0.33874777]\n",
      " [0.32748706 0.33314924 0.3393637 ]\n",
      " [0.32684218 0.33329521 0.33986262]\n",
      " [0.32800366 0.33257214 0.3394242 ]\n",
      " [0.32575198 0.33301726 0.34123076]\n",
      " [0.33789575 0.33035647 0.33174778]\n",
      " [0.32537525 0.33313288 0.34149188]\n",
      " [0.32902955 0.33273559 0.33823487]\n",
      " [0.32604006 0.33318265 0.34077729]\n",
      " [0.33034892 0.33295425 0.33669684]\n",
      " [0.32967049 0.33303689 0.33729262]\n",
      " [0.32861324 0.33280854 0.33857822]\n",
      " [0.33516012 0.33250606 0.33233381]\n",
      " [0.32810138 0.332936   0.33896262]\n",
      " [0.32905296 0.33325006 0.33769698]\n",
      " [0.3375636  0.33048775 0.33194865]\n",
      " [0.32995378 0.33262729 0.33741893]\n",
      " [0.32422619 0.33355411 0.34221969]\n",
      " [0.32570445 0.33291122 0.34138433]\n",
      " [0.33727952 0.33055567 0.33216481]\n",
      " [0.3309658  0.33316516 0.33586903]\n",
      " [0.32472347 0.33304763 0.3422289 ]\n",
      " [0.32560916 0.33321444 0.3411764 ]\n",
      " [0.33685179 0.33131706 0.33183115]\n",
      " [0.32462669 0.33327907 0.34209424]\n",
      " [0.33648537 0.33147721 0.33203742]\n",
      " [0.32741039 0.33280022 0.33978939]\n",
      " [0.32387316 0.33368623 0.34244061]\n",
      " [0.32606585 0.33335285 0.3405813 ]\n",
      " [0.32938997 0.33292377 0.33768626]\n",
      " [0.32619431 0.33318833 0.34061736]\n",
      " [0.32727506 0.3337516  0.33897333]\n",
      " [0.32865105 0.33285316 0.33849579]\n",
      " [0.3268728  0.33337148 0.33975572]\n",
      " [0.32727804 0.33306964 0.33965231]\n",
      " [0.33653408 0.33099009 0.33247583]\n",
      " [0.32907852 0.33291254 0.33800895]\n",
      " [0.32811644 0.3335797  0.33830385]\n",
      " [0.3380648  0.32998849 0.33194671]\n",
      " [0.32918568 0.33315862 0.33765569]\n",
      " [0.32437698 0.3334069  0.34221612]]\n",
      "y_true : [0 2 2 0 1 1 2 1 2 0 2 1 2 1 1 1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1 1\n",
      " 2 2 0 1 2 0 1 2]\n",
      "y_pred : [0 2 2 0 2 2 2 2 2 0 2 2 2 2 2 2 0 2 2 0 2 2 2 0 2 2 2 0 2 0 2 2 2 2 2 2 2\n",
      " 2 2 0 2 2 0 2 2]\n",
      "Logistic Regression Accuracy: 0.6222222222222222\n"
     ]
    }
   ],
   "source": [
    "# driver code for linear model in iris dataset\n",
    "logistic_regression = LinearModel(model_type=\"logistic\")\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "y_pred = logistic_regression.predict(X_test)\n",
    "\n",
    "# print(f'y_pred = {y_pred}\\ny_test = {y_test}')\n",
    "print(\"Logistic Regression Accuracy:\", accuracy(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boston, \"medv\")\n",
    "X_train, X_test = normalize(X_train), normalize(X_test)\n",
    "\n",
    "linear_regression = LinearModel(model_type=\"linear\")\n",
    "linear_regression.fit(X_train, y_train)\n",
    "y_pred = linear_regression.predict(X_test)\n",
    "print(\"Linear Regression MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------doesn't matter -----------\n",
    "\n",
    "Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth: int = 5, model_type: str = \"classifier\"):\n",
    "        self.max_depth = max_depth\n",
    "        self.model_type = model_type\n",
    "\n",
    "        assert model_type in [\n",
    "            \"classifier\",\n",
    "            \"regressor\",\n",
    "        ], \"model_type must be either 'classifier' or 'regressor'\"\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        self.tree = self._build_tree(X, y, 0)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _build_tree(self, X: np.ndarray, y: np.ndarray, depth: int) -> dict:\n",
    "        if depth >= self.max_depth or self._is_pure(y):\n",
    "            return self._create_leaf(y)\n",
    "\n",
    "        feature, threshold = self._find_best_split(X, y)\n",
    "        mask = X[:, feature] <= threshold\n",
    "        X_left, y_left = X[mask], y[mask]\n",
    "        X_right, y_right = X[~mask], y[~mask]\n",
    "\n",
    "        left_child = self._build_tree(X_left, y_left, depth + 1)\n",
    "        right_child = self._build_tree(X_right, y_right, depth + 1)\n",
    "\n",
    "        return {\n",
    "            \"feature\": feature,\n",
    "            \"threshold\": threshold,\n",
    "            \"left\": left_child,\n",
    "            \"right\": right_child,\n",
    "        }\n",
    "\n",
    "    def _is_pure(self, y: np.ndarray) -> bool:\n",
    "        return len(set(y)) == 1\n",
    "\n",
    "    def _create_leaf(self, y: np.ndarray):\n",
    "        if self.model_type == \"classifier\":\n",
    "            return np.bincount(y).argmax()\n",
    "        else:\n",
    "            return np.mean(y)\n",
    "\n",
    "    def _find_best_split(self, X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n",
    "        best_gini = float(\"inf\")\n",
    "        best_mse = float(\"inf\")\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            sorted_indices = np.argsort(X[:, feature])\n",
    "            for i in range(1, len(X)):\n",
    "                if X[sorted_indices[i - 1], feature] != X[sorted_indices[i], feature]:\n",
    "                    threshold = (\n",
    "                        X[sorted_indices[i - 1], feature]\n",
    "                        + X[sorted_indices[i], feature]\n",
    "                    ) / 2\n",
    "                    mask = X[:, feature] <= threshold\n",
    "                    left_y, right_y = y[mask], y[~mask]\n",
    "\n",
    "                    if self.model_type == \"classifier\":\n",
    "                        gini = self._gini_index(left_y, right_y)\n",
    "                        if gini < best_gini:\n",
    "                            best_gini = gini\n",
    "                            best_feature = feature\n",
    "                            best_threshold = threshold\n",
    "                    else:\n",
    "                        mse = self._mse(left_y, right_y)\n",
    "                        if mse < best_mse:\n",
    "                            best_mse = mse\n",
    "                            best_feature = feature\n",
    "                            best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _gini_index(self, left_y: np.ndarray, right_y: np.ndarray) -> float:\n",
    "        unique_classes = np.unique(np.concatenate((left_y, right_y)))\n",
    "        gini_index = 0.0\n",
    "\n",
    "        for class_val in unique_classes:\n",
    "            p_left = len(left_y[left_y == class_val]) / len(left_y)\n",
    "            p_right = len(right_y[right_y == class_val]) / len(right_y)\n",
    "            gini_left = 1 - p_left ** 2 - (1 - p_left) ** 2\n",
    "            gini_right = 1 - p_right ** 2 - (1 - p_right) ** 2\n",
    "            weighted_gini = (len(left_y) * gini_left + len(right_y) * gini_right) / (\n",
    "                len(left_y) + len(right_y)\n",
    "            )\n",
    "            gini_index += weighted_gini\n",
    "\n",
    "        return gini_index\n",
    "\n",
    "    def _mse(self, left_y: np.ndarray, right_y: np.ndarray) -> float:\n",
    "        mean_left = np.mean(left_y)\n",
    "        mean_right = np.mean(right_y)\n",
    "        mse_left = np.mean((left_y - mean_left) ** 2)\n",
    "        mse_right = np.mean((right_y - mean_right) ** 2)\n",
    "        weighted_mse = (len(left_y) * mse_left + len(right_y) * mse_right) / (len(left_y) + len(right_y) )\n",
    "\n",
    "        return weighted_mse\n",
    "    \n",
    "    def _traverse_tree(self, x: np.ndarray, node: dict):\n",
    "        if isinstance(node, dict):\n",
    "            feature, threshold = node[\"feature\"], node[\"threshold\"]\n",
    "            if x[feature] <= threshold:\n",
    "                return self._traverse_tree(x, node[\"left\"])\n",
    "            else:\n",
    "                return self._traverse_tree(x, node[\"right\"])\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Accuracy: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "decision_tree_classifier = DecisionTree(model_type=\"classifier\")\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "y_pred = decision_tree_classifier.predict(X_test)\n",
    "print(\"Decision Tree Classifier Accuracy:\", accuracy(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    boston "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regressor MSE: 28.341570306709183\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boston, \"medv\")\n",
    "X_train, X_test = normalize(X_train), normalize(X_test)\n",
    "\n",
    "decision_tree_regressor = DecisionTree(model_type=\"regressor\")\n",
    "decision_tree_regressor.fit(X_train, y_train)\n",
    "y_pred = decision_tree_regressor.predict(X_test)\n",
    "print(\"Decision Tree Regressor MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators: int = 100, max_depth: int = 5, model_type: str = \"classifier\"):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.model_type = model_type\n",
    "        # Initialize a list of DecisionTree instances based on the specified number of estimators, max depth, and model type\n",
    "        self.trees = [ DecisionTree(max_depth=max_depth, model_type=model_type) for _ in range(n_estimators)]\n",
    "\n",
    "        assert model_type in [\"classifier\", \"regressor\"], \"model_type must be either 'classifier' or 'regressor'\"\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, detail = False ) -> None:\n",
    "        iter = 1 \n",
    "        for tree in tqdm(self.trees):\n",
    "            # Generate bootstrap indices by random sampling with replacement\n",
    "            bootstrap_indices = np.random.choice(len(X), len(X), replace=True)\n",
    "            bootstrap_X, bootstrap_y = X[bootstrap_indices], y[bootstrap_indices]\n",
    "            \n",
    "            # Fit each tree with the corresponding samples from X and y\n",
    "            tree.fit(bootstrap_X, bootstrap_y)\n",
    "            if (iter % 10 == 0 and detail):\n",
    "                print(f'Finished {iter} iters')\n",
    "            iter += 1\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        predictions = []\n",
    "\n",
    "        for tree in self.trees:\n",
    "            # Predict the output for each tree\n",
    "            tree_pred = tree.predict(X)\n",
    "            predictions.append(tree_pred)\n",
    "\n",
    "        if self.model_type == \"classifier\":\n",
    "            # Majority voting for classification\n",
    "            predictions = np.array(predictions)\n",
    "            majority_votes = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=predictions)\n",
    "            return majority_votes\n",
    "        else:\n",
    "            # Averaging for regression\n",
    "            predictions = np.array(predictions)\n",
    "            return np.mean(predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 224.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor MSE: 0.04437957998163452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest_regressor = RandomForest(model_type=\"regressor\", n_estimators= 100)\n",
    "random_forest_regressor.fit(X_train, y_train)\n",
    "y_pred = random_forest_regressor.predict(X_test)\n",
    "print(\"Random Forest Regressor MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier Accuracy: 0.8888888888888888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 276.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy: 0.9111111111111111\n",
      "Decision Tree Regressor MSE: 28.341570306709183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:14<00:00,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor MSE: 23.60490294302168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris, \"class\")\n",
    "X_train, X_test = normalize(X_train), normalize(X_test)\n",
    "y_train, y_test = encode_labels(y_train), encode_labels(y_test)\n",
    "\n",
    "decision_tree_classifier = DecisionTree(model_type=\"classifier\")\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "y_pred = decision_tree_classifier.predict(X_test)\n",
    "print(\"Decision Tree Classifier Accuracy:\", accuracy(y_test, y_pred))\n",
    "\n",
    "random_forest_classifier = RandomForest(model_type=\"classifier\")\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "y_pred = random_forest_classifier.predict(X_test)\n",
    "print(\"Random Forest Classifier Accuracy:\", accuracy(y_test, y_pred))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston, \"medv\")\n",
    "X_train, X_test = normalize(X_train), normalize(X_test)\n",
    "\n",
    "decision_tree_regressor = DecisionTree(model_type=\"regressor\")\n",
    "decision_tree_regressor.fit(X_train, y_train)\n",
    "y_pred = decision_tree_regressor.predict(X_test)\n",
    "print(\"Decision Tree Regressor MSE:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "random_forest_regressor = RandomForest(model_type=\"regressor\")\n",
    "random_forest_regressor.fit(X_train, y_train)\n",
    "y_pred = random_forest_regressor.predict(X_test)\n",
    "print(\"Random Forest Regressor MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
