{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW word2vec model\n",
    "CBOW model predicts the center word given the context words.\n",
    "\n",
    "The input to the model is the word IDs for the context words. These word IDs are fed into a common embedding layer that is initialized with small random weights. Each word ID is transformed into a vector of size (embed_size) by the embedding layer.\n",
    "\n",
    "Thus, each row of the input context is transformed into a matrix of size (2*window_size, embed_size) by this layer.\n",
    "\n",
    "This is fed into a lambda layer, which computes an average of all the embeddings. This average is then fed to a dense layer, which creates a dense vector of size (vocab_size) for each row.\n",
    "\n",
    "![CBOW model](https://i.imgur.com/iJ0H699.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "# https://gist.github.com/phillipj/4944029\n",
    "fin = open(\"alice_in_wonderland.txt\", \"r\", encoding = 'utf-8')\n",
    "#############################################################################\n",
    "# TODO: Write a for loop to read in the corpus.                             #\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for line in fin:\n",
    "#     line = line.strip()\n",
    "#     if len(line) == 0:\n",
    "#         continue\n",
    "#     lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.sent_tokenize(\" \".join(lines))\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Use tokenizer to tokenize the corpus.                               #\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "w_lefts, w_centers, w_rights = [], [], []\n",
    "for sent in sents:\n",
    "#############################################################################\n",
    "# TODO: Create training data for cbow.                                      #\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(5000)  # use top 5000 words only\n",
    "# tokens = tokenizer.fit_on_texts(sents)\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "#     embedding = one_hot(sent, vocab_size)\n",
    "#     triples = list(nltk.trigrams(embedding))\n",
    "#     w_lefts.extend([x[0] for x in triples])\n",
    "#     w_centers.extend([x[1] for x in triples])\n",
    "#     w_rights.extend([x[2] for x in triples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(n_values=vocab_size)\n",
    "#############################################################################\n",
    "# TODO: Use one hot encoder to fit on the corpus.                           #\n",
    "#############################################################################\n",
    "\n",
    "#############################################################################\n",
    "#                          END OF YOUR CODE                                 #\n",
    "#############################################################################\n",
    "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(Xtrain.shape[1],)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(Ytrain.shape[1]))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, \n",
    "                    epochs=NUM_EPOCHS, verbose=1,\n",
    "                    validation_data=(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xleft = ohe.fit_transform(np.array(w_lefts).reshape(-1, 1)).todense()\n",
    "# Xright = ohe.fit_transform(np.array(w_rights).reshape(-1, 1)).todense()\n",
    "# X = (Xleft + Xright) / 2.0\n",
    "# Y = ohe.fit_transform(np.array(w_centers).reshape(-1, 1)).todense()\n",
    "# Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3,\n",
    "#                                                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss function\n",
    "plt.subplot(211)\n",
    "plt.title(\"accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"r\", label=\"train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"r\", label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score = model.evaluate(Xtest, Ytest, verbose=1)\n",
    "print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))\n",
    "\n",
    "# using the word2vec model\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "\n",
    "# retrieve the weights from the first dense layer. This will convert\n",
    "# the input vector from a one-hot sum of two words to a dense 300 \n",
    "# dimensional representation\n",
    "W, b = model.layers[0].get_weights()\n",
    "\n",
    "idx2emb = {}    \n",
    "for word in word2idx.keys():\n",
    "    wid = word2idx[word]\n",
    "    vec_in = ohe.fit_transform(np.array(wid)).todense()\n",
    "    vec_emb = np.dot(vec_in, W)\n",
    "    idx2emb[wid] = vec_emb\n",
    "\n",
    "for word in [\"stupid\", \"alice\", \"succeeded\"]:\n",
    "    wid = word2idx[word]\n",
    "    source_emb = idx2emb[wid]\n",
    "    distances = []\n",
    "    for i in range(1, vocab_size):\n",
    "        if i == wid:\n",
    "            continue\n",
    "        target_emb = idx2emb[i]\n",
    "        distances.append(((wid, i), \n",
    "                         cosine_distances(source_emb, target_emb)))\n",
    "    sorted_distances = sorted(distances, key=operator.itemgetter(1))[0:10]\n",
    "    predictions = [idx2word[x[0][1]] for x in sorted_distances]\n",
    "    print(\"{:s} => {:s}\".format(word, \", \".join(predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
